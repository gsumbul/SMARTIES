{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108aa376",
   "metadata": {},
   "source": [
    "# Getting Started | SMARTIES Hugging Face Transformer Model \n",
    "This notebook demonstrates the use of SMARTIES pretrained models using the Hugging Face Transformers interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7139c2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To use SMARTIES model weights with HF transformers interface, you need to install only Transformers Python package ($\\geq$ v4.52.0). For this notebook, EuroSAT dataset from torchgeo is used as an example of downstream task/dataset. This requires to install [torchgeo](https://github.com/microsoft/torchgeo) package. To install all the required packages of this notebook, you can run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torchgeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0724bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchgeo.datasets import EuroSAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe340c34",
   "metadata": {},
   "source": [
    "## Load SMARTIES Weights\n",
    "With one line of code, you can load the SMARTIES model weights from Hugging Face Hub. The model is loaded in evaluation mode by default. There are two versions of SMARTIES model available on Hugging Face Hub, one with ViT-B backbone and the other with ViT-L backbone. You can choose the one that fits your needs with model name: \n",
    "```python\n",
    "'gsumbul/SMARTIES-v1-ViT-B' or 'gsumbul/SMARTIES-v1-ViT-L'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"gsumbul/SMARTIES-v1-ViT-B\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab6b98",
   "metadata": {},
   "source": [
    "## Prepare Dataloader\n",
    "\n",
    "In SMARTIES paper, data preprocessing is achieved by first min-max image normalization with 1% and 99% percentile values, and then image standardization with mean and standard deviation values (calculated after the first step). This allows SMARTIES to be robust towards data distribution differences across multiple sensors (e.g., long-tailed distribution of 12 bit Sentinel-2 images vs. short-tailed distribution of 8 bit RGB images). However, you can also use SMARTIES with only widely used mean-std standardization, expecting a slight drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercentileNormalize(torch.nn.Module):\n",
    "    def __init__(self, percentile1, percentile99):\n",
    "        super().__init__()\n",
    "        self.percentile1 = torch.tensor(percentile1)\n",
    "        self.percentile99 = torch.tensor(percentile99)\n",
    "    def forward(self, inpts):\n",
    "        image, label = inpts['image'], inpts['label']\n",
    "        return {\n",
    "            'image': image.sub_(self.percentile1.view(-1, 1, 1)).div_((self.percentile99 - self.percentile1).view(-1, 1, 1)).clamp_(min=0,max=1),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# EuroSAT dataset initialization\n",
    "# You may need to adjust the root path and download/checksum flags as needed\n",
    "dataset = lambda split: EuroSAT(\n",
    "    root=\"EuroSAT\",\n",
    "    split=split,\n",
    "    bands=('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12'),\n",
    "    transforms=v2.Compose([\n",
    "        PercentileNormalize(percentile1=[968.0, 697.0, 457.0, 242.0, 203.0, 179.0, 158.0, 131.0, 119.0, 57.0, 22.0, 11.0],\n",
    "                            percentile99=[2084.0, 2223.0, 2321.0, 2862.0, 2883.0, 3898.0, 4876.0, 4806.0, 5312.0, 1851.0, 4205.0, 3132.0]),\n",
    "        v2.Normalize(\n",
    "            mean=[0.34366747736930847, 0.2713719308376312, 0.3102375864982605, 0.2662188410758972, 0.36944717168807983, 0.4893955886363983, 0.4686998128890991, 0.46322500705718994, 0.4768053889274597, 0.3750271201133728, 0.42840376496315, 0.3525424003601074],\n",
    "            std=[0.21045532822608948, 0.1970716118812561, 0.19605565071105957, 0.21756012737751007, 0.20496250689029694, 0.22960464656352997, 0.22847740352153778, 0.23722581565380096, 0.23559165000915527, 0.22142820060253143, 0.23700211942195892, 0.23857484757900238],\n",
    "        ),\n",
    "        v2.Resize((224, 224))\n",
    "    ]),\n",
    "    download=True,\n",
    ")\n",
    "train_ds, val_ds = dataset('train'), dataset('val')\n",
    "\n",
    "# You may need to adjust the batch size and number of workers based on your system's capabilities\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=True, num_workers=8)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "image = batch[\"image\"]\n",
    "label = batch[\"label\"]\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd3772",
   "metadata": {},
   "source": [
    "## Downstream Transfer\n",
    "SMARTIES enables sensor-agnostic processing of remote sensing (RS) data, and thus downstream transfer using a unified model across a diverse set of RS sensors and tasks while allowing for the use of arbitrary combinations of spectral bands. To do so, you need to specify which spectrum-aware projection layers you want to use for downstream transfer. Each projection layer is associated with a unique range in the electromagnetic spectrum (i.e., spectral band) that is defined in the `spectrum_specs.yaml` file with keys as band names (e.g., aerosal, red_edge_1, blue_1 etc.). SMARTIES supports a wide range of spectral bands, and you can choose the bands that are relevant to your specific application.  \n",
    "\n",
    "To set the bands you want to use, you can either: \n",
    "1. Use the `sensor_type` parameter for predefined bands of well-known sensory input. It can be set as `S2` for Sentinel-2 L2A image bands, `S1` for Sentinel-1 GRD image bands, or `RGB` for VHR commercial RGB image bands. \n",
    "   - Example: `model(image, sensor_type='S2')`\n",
    "2. Specify bands by passing a list of band names to the `bands` parameter when calling the model. Note that the order of the bands in the list should match the order of the bands in the input image tensor. \n",
    "    - Example: `model(image, bands=['blue_1', 'green_1', 'red_1','near_infrared_1'])`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802be67",
   "metadata": {},
   "source": [
    "### Image-Level Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86714d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As bands of the dataset follow Sentinel-2 L2A image bands, sensor-type parameter can be used\n",
    "image_feats = model(image, sensor_type='S2')\n",
    "print(\"Features shape:\", image_feats.shape)\n",
    "\n",
    "# Or you can specify the bands you want to use directly\n",
    "# All the bands used\n",
    "all_bands = ['aerosol', 'blue_1', 'green_2', 'red_2', 'red_edge_1', 'red_edge_2', 'near_infrared_2', 'near_infrared_1', \n",
    "         'near_infrared_3', 'short_wave_infrared_1', 'short_wave_infrared_3', 'short_wave_infrared_4']\n",
    "\n",
    "image_feats = model(image, bands=all_bands)\n",
    "print(\"Features shape:\", image_feats.shape)\n",
    "\n",
    "# Only RGB bands used \n",
    "rgb_bands = ['blue_1', 'green_2', 'red_2']\n",
    "image_rgb_feats = model(image[:,[1,2,3]], bands=rgb_bands)\n",
    "print(\"Features shape:\", image_rgb_feats.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e10871",
   "metadata": {},
   "source": [
    "### Dense Feature Extraction with All Tokens\n",
    "For dense tasks (e.g. semantic segmentation) during downstream transfer, features associated with all the tokens (including CLS token) can be extracted by setting `all_tokens=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e85523",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feats_dense = model(image, bands=all_bands, all_tokens=True)\n",
    "print(\"Features shape (all tokens):\", image_feats_dense.shape)\n",
    "print(\"Features shape (all tokens without cls token):\", image_feats_dense[:,1:,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0209be96",
   "metadata": {},
   "source": [
    "### Linear Probing for Scene-Classification\n",
    "\n",
    "To perform scene classification on EuroSAT as an example of downstream transfer, in this notebook, linear probing is employed. To do so, we freeze the SMARTIES encoder and train a single FC (linear) layer on top for classification. This is done by extracting features for each image and training a linear classifier using these features.\n",
    "\n",
    "Below, we show how to set up, train, and evaluate linear probing on top of SMARTIES features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd88be6",
   "metadata": {},
   "source": [
    "#### Scene Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7feada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Freeze SMARTIES encoder\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get feature dimension from a dummy forward pass\n",
    "with torch.no_grad():\n",
    "    dummy_feats = model(image.to(device), sensor_type='S2')\n",
    "    feat_dim = dummy_feats.shape[-1]\n",
    "\n",
    "num_classes = len(train_ds.classes)\n",
    "\n",
    "class SceneClassification(nn.Module):\n",
    "    def __init__(self, backbone, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(feat_dim, affine=False, eps=1e-6), \n",
    "            nn.Linear(feat_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x, **kwargs):\n",
    "        feats = self.backbone(x, **kwargs)\n",
    "        return self.head(feats)\n",
    "\n",
    "cls_model = SceneClassification(model, feat_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1035184",
   "metadata": {},
   "source": [
    "#### Training Loop for Linear Probing\n",
    "We train only the linear layer while keeping the backbone frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(cls_model.head.parameters(), lr=1e-3, betas=(0.9, 0.95))\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for batch in dataloader:\n",
    "        imgs, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, sensor_type='S2')\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        total_samples += imgs.size(0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss, acc = train_one_epoch(cls_model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8ae49",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Evaluate the linear probe on the validation set of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "            outputs = model(imgs, sensor_type='S2')\n",
    "            preds = outputs.argmax(1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += imgs.size(0)\n",
    "    acc = total_correct / total_samples\n",
    "    print(f\"Evaluation Accuracy: {acc:.4f}\")\n",
    "\n",
    "evaluate(cls_model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
